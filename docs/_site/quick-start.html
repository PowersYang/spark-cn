
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Quick Start - Spark 3.0.0 Documentation</title>
        
          <meta name="description" content="Quick start tutorial for Spark 3.0.0">
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">3.0.0</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">概述</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL, DataFrames, 以及 Datasets</a></li>
                                <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                                <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                                <li><a href="ml-guide.html">MLlib (机器学习)</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX (图形处理)</a></li>
                                <li><a href="sparkr.html">SparkR (Spark上的R)</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API 文档<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                                <li><a href="api/java/index.html">Java</a></li>
                                <li><a href="api/python/index.html">Python</a></li>
                                <li><a href="api/R/index.html">R</a></li>
                                <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">概述</a></li>
                                <li><a href="submitting-applications.html">提交应用</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark Standalone</a></li>
                                <li><a href="running-on-mesos.html">Mesos</a></li>
                                <li><a href="running-on-yarn.html">YARN</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">配置</a></li>
                                <li><a href="monitoring.html">监控</a></li>
                                <li><a href="tuning.html">调优直男</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li><a href="migration-guide.html">迁移至南</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">构建Spark</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                        <li class="dropdown"><a href="http://www.ysir308.com/aboutme" target="_blank">贡献翻译</a></li>
                        <li class="dropdown"><a href="http://www.ysir308.com/aboutme" target="_blank">联系站长</a></li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">Quick Start</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#安全" id="markdown-toc-安全">安全</a></li>
  <li><a href="#使用spark-shell进行交互式分析" id="markdown-toc-使用spark-shell进行交互式分析">使用Spark Shell进行交互式分析</a>    <ul>
      <li><a href="#基础" id="markdown-toc-基础">基础</a></li>
      <li><a href="#有关数据集操作的更多信息" id="markdown-toc-有关数据集操作的更多信息">有关数据集操作的更多信息</a></li>
      <li><a href="#缓存" id="markdown-toc-缓存">缓存</a></li>
    </ul>
  </li>
  <li><a href="#独立的应用" id="markdown-toc-独立的应用">独立的应用</a></li>
  <li><a href="#下一步做什么" id="markdown-toc-下一步做什么">下一步做什么</a></li>
</ul>
<p>本教程提供了使用Spark的快速介绍。我们将首先通过Spark的交互式Shell（在Python或Scala中）介绍API，然后展示如何用Java，Scala和Python编写应用程序。</p>

<p>要学习本指南，请首先从<a href="https://spark.apache.org/downloads.html">Spark网站</a>下载Spark的package版本 。由于我们不会使用HDFS，因此你可以下载任何版本的Hadoop。</p>

<p>请注意，在Spark 2.0之前，Spark的主要编程接口是弹性分布式数据集（RDD）。在Spark 2.0之后，RDD被Dataset取代，Dataset的类型像RDD一样强，但具有更丰富的优化功能。仍支持RDD界面，你可以在<a href="rdd-programming-guide.html">RDD编程指南中</a>获得更详细的参考。但是，我们强烈建议你切换到使用数据集，该数据集的性能比RDD更好。请参阅《<a href="sql-programming-guide.html">SQL编程指南》</a>以获取有关数据集的更多信息。</p>

<h1 id="安全">安全</h1>

<p>默认情况下，Spark中的安全性处于关闭状态。这可能意味着你默认情况下容易受到攻击。运行Spark之前，请参阅<a href="security.html">Spark Security</a>。</p>

<h1 id="使用spark-shell进行交互式分析">使用Spark Shell进行交互式分析</h1>

<h2 id="基础">基础</h2>

<p>Spark shell提供了学习API的简单方法，以及强大的工具来交互式地分析数据。它可以在Scala（可在Java VM上运行，因此是使用现有Java库的好方法）或Python中提供。通过在Spark目录中运行以下命令来启动它：</p>

<div class="codetabs">
<div data-lang="scala">

    <pre><code>./bin/spark-shell
</code></pre>

    <p>Spark的主要抽象是称为Dataset的分布式集合。可以从Hadoop InputFormats（例如HDFS文件）或通过转换其他数据集来创建DataSet。让我们从Spark源目录中的README文件的文本中创建一个新的数据集：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">textFile</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"README.md"</span><span class="o">)</span>
<span class="n">textFile</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.Dataset</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">value:</span> <span class="kt">string</span><span class="o">]</span></code></pre></figure>

    <p>你可以通过调用某些操作直接从数据集中获取值，或转换数据集以获取新值。有关更多详细信息，请阅读<em><a href="api/scala/index.html#org.apache.spark.sql.Dataset">API文档</a></em>。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">count</span><span class="o">()</span> <span class="c1">// Number of items in this Dataset
</span><span class="n">res0</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">126</span> <span class="c1">// May be different from yours as README.md will change over time, similar to other outputs
</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">first</span><span class="o">()</span> <span class="c1">// First item in this Dataset
</span><span class="n">res1</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="k">#</span> <span class="nc">Apache</span> <span class="nc">Spark</span></code></pre></figure>

    <p>现在，让我们将此数据集转换为新的数据集。我们调用<code>filter</code>返回一个新的数据集，其中包含文件中项的子集。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">linesWithSpark</span> <span class="k">=</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">contains</span><span class="o">(</span><span class="s">"Spark"</span><span class="o">))</span>
<span class="n">linesWithSpark</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.Dataset</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">value:</span> <span class="kt">string</span><span class="o">]</span></code></pre></figure>

    <p>我们可以将转换和动作链接在一起：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">contains</span><span class="o">(</span><span class="s">"Spark"</span><span class="o">)).</span><span class="py">count</span><span class="o">()</span> <span class="c1">// How many lines contain "Spark"?
</span><span class="n">res3</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">15</span></code></pre></figure>

  </div>
<div data-lang="python">

    <pre><code>./bin/pyspark
</code></pre>

    <p>或者在当前环境中使用pip安装PySpark的包：</p>

    <pre><code>pyspark
</code></pre>

    <p>Spark的主要抽象是称为数据集的项目的分布式集合。可以从Hadoop InputFormats（例如HDFS文件）或通过转换其他数据集来创建数据集。由于Python的动态性质，我们不需要在Python中对数据集进行强类型化。Python中的所有数据集都是Dataset [Row]，我们称其<code>DataFrame</code>与Pandas和R中的数据框概念一致。让我们从Spark源目录中的README文件的文本中创建一个新的DataFrame：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">textFile</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s">"README.md"</span><span class="p">)</span></code></pre></figure>

    <p>你可以通过调用一些操作直接从DataFrame中获取值，或转换DataFrame以获取新的值。有关更多详细信息，请阅读<em><a href="api/python/index.html#pyspark.sql.DataFrame">API文档</a></em>。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">textFile</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>  <span class="c1"># Number of rows in this DataFrame
</span><span class="mi">126</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">textFile</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>  <span class="c1"># First row in this DataFrame
</span><span class="n">Row</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s">u'# Apache Spark'</span><span class="p">)</span></code></pre></figure>

    <p>现在，让我们将此DataFrame转换为一个新的。我们调用<code>filter</code>返回一个新的DataFrame，其中包含文件中各行的子集。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">linesWithSpark</span> <span class="o">=</span> <span class="n">textFile</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">textFile</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">"Spark"</span><span class="p">))</span></code></pre></figure>

    <p>我们可以将转换和动作链接在一起：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">textFile</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">textFile</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">"Spark"</span><span class="p">))</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>  <span class="c1"># How many lines contain "Spark"?
</span><span class="mi">15</span></code></pre></figure>

  </div>
</div>

<h2 id="有关数据集操作的更多信息">有关数据集操作的更多信息</h2>

<p>DataSet动作和转换可用于更复杂的计算。假设我们要查找包含最多单词的行：</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">).</span><span class="py">size</span><span class="o">).</span><span class="py">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nf">if</span> <span class="o">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="o">)</span> <span class="n">a</span> <span class="k">else</span> <span class="n">b</span><span class="o">)</span>
<span class="n">res4</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">15</span></code></pre></figure>

    <p>首先，将一行内容映射到一个整数值以创建一个新的DataSet。<code>reduce</code>在该数据集上调用，以找到最大的字数。传给<code>map</code>和<code>reduce</code>的参数是Scala的闭包函数，同时也可以使用Scala和Java的库函数。例如，我们可以轻松地在其他地方调用声明的函数。我们将使用<code>Math.max()</code>函数使此代码更易于理解：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nn">java.lang.Math</span>
<span class="k">import</span> <span class="nn">java.lang.Math</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">).</span><span class="py">size</span><span class="o">).</span><span class="py">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nv">Math</span><span class="o">.</span><span class="py">max</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">))</span>
<span class="n">res5</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">15</span></code></pre></figure>

    <p>一种常见的数据流模式是Hadoop流行的MapReduce。Spark可以轻松实现MapReduce流：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">wordCounts</span> <span class="k">=</span> <span class="nv">textFile</span><span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="py">groupByKey</span><span class="o">(</span><span class="n">identity</span><span class="o">).</span><span class="py">count</span><span class="o">()</span>
<span class="n">wordCounts</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.Dataset</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">value:</span> <span class="kt">string</span>, <span class="kt">count</span><span class="o">(</span><span class="err">1</span><span class="o">)</span><span class="kt">:</span> <span class="kt">bigint</span><span class="o">]</span></code></pre></figure>

    <p>在这里，我们调用<code>flatMap</code>将行的数据集转换为单词的数据集，然后组合<code>groupByKey</code>并使用<code>count</code>来计算文件中每个单词的计数，最后返回（String, Long）类型的数据对。要收集shell中的数据，可以调用<code>collect</code>：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="nv">wordCounts</span><span class="o">.</span><span class="py">collect</span><span class="o">()</span>
<span class="n">res6</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">((</span><span class="n">means</span><span class="o">,</span><span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="n">under</span><span class="o">,</span><span class="mi">2</span><span class="o">),</span> <span class="o">(</span><span class="k">this</span><span class="o">,</span><span class="mi">3</span><span class="o">),</span> <span class="o">(</span><span class="nc">Because</span><span class="o">,</span><span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="nc">Python</span><span class="o">,</span><span class="mi">2</span><span class="o">),</span> <span class="o">(</span><span class="n">agree</span><span class="o">,</span><span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="n">cluster</span><span class="o">.,</span><span class="mi">1</span><span class="o">),</span> <span class="o">...)</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">textFile</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="n">textFile</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s">"</span><span class="err">\</span><span class="s">s+"</span><span class="p">))</span><span class="o">.</span><span class="n">name</span><span class="p">(</span><span class="s">"numWords"</span><span class="p">))</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"numWords"</span><span class="p">)))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">numWords</span><span class="p">)</span><span class="o">=</span><span class="mi">15</span><span class="p">)]</span></code></pre></figure>

    <p>首先，将一行内容映射到一个整数值，并将其取名为“ numWords”，从而创建一个新的DataFrame。在该DataFrame上调用<code>agg</code>以找到最大的字数。传给<code>select</code>和<code>agg</code>的参数都是<em><a href="api/python/index.html#pyspark.sql.Column">Column对象</a></em>，我们可以使用<code>df.colName</code>从DataFrame中获得一列。我们还可以导入pyspark.sql.functions，它提供了许多从旧的列构建新列的函数。</p>

    <p>一种常见的数据流模式是Hadoop流行的MapReduce。Spark可以轻松实现MapReduce流</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">wordCounts</span> <span class="o">=</span> <span class="n">textFile</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="n">textFile</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s">"</span><span class="err">\</span><span class="s">s+"</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">"word"</span><span class="p">))</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">"word"</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span></code></pre></figure>

    <p>这里我们在 <code>select</code>中使用了 <code>explode</code> 函数将行的数据集转换为单词的数据集，然后结合 <code>groupBy</code> 和 <code>count</code> 来计算每一个单词在文件中出现的次数，最后输出一个包含两个Column（&#8221;word&#8221; and &#8220;count&#8221;）的DataFrame。 可以使用 <code>collect</code>函数收集计算结果：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="s">u'online'</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="s">u'graphs'</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="o">...</span><span class="p">]</span></code></pre></figure>

  </div>
</div>

<h2 id="缓存">缓存</h2>

<p>Spark还支持将数据集提取到集群范围的内存缓存中。当重复访问比较小的数据集或运行迭代算法（如PageRank）时，这非常有用。举一个简单的例子，让我们标记<code>linesWithSpark</code>要缓存的数据集：</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="nv">linesWithSpark</span><span class="o">.</span><span class="py">cache</span><span class="o">()</span>
<span class="n">res7</span><span class="k">:</span> <span class="kt">linesWithSpark.</span><span class="k">type</span> <span class="o">=</span> <span class="o">[</span><span class="kt">value:</span> <span class="kt">string</span><span class="o">]</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="nv">linesWithSpark</span><span class="o">.</span><span class="py">count</span><span class="o">()</span>
<span class="n">res8</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">15</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="nv">linesWithSpark</span><span class="o">.</span><span class="py">count</span><span class="o">()</span>
<span class="n">res9</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">15</span></code></pre></figure>

    <p>使用Spark计算和缓存100行文本文件似乎很愚蠢。有趣的是，即使在数十或数百个节点上，这些相同的函数也可以用于非常大的数据集。你也可以<code>bin/spark-shell</code>按照<a href="rdd-programming-guide.html#using-the-shell">RDD编程指南</a>中的说明，通过连接到计算以交互的方式进行操作。</p>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">linesWithSpark</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">linesWithSpark</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="mi">15</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">linesWithSpark</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="mi">15</span></code></pre></figure>

    <p>使用Spark计算和缓存100行文本文件似乎很愚蠢。有趣的是，即使在数十或数百个节点上，这些相同的函数也可以用于非常大的数据集。你也可以<code>bin/pyspark</code>按照<a href="rdd-programming-guide.html#using-the-shell">RDD编程指南</a>中的说明，通过连接到计算以交互的方式进行操作。</p>

  </div>
</div>

<h1 id="独立的应用">独立的应用</h1>
<p>假设我们希望使用Spark API编写一个独立的应用程序。我们将逐步介绍一个Scala（带有sbt），Java（带有Maven）和Python（pip）的简单应用程序。</p>

<div class="codetabs">
<div data-lang="scala">
    <p>我们将在Scala中创建一个非常简单的Spark应用程序，名为<code>SimpleApp.scala</code>：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="cm">/* SimpleApp.scala */</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">object</span> <span class="nc">SimpleApp</span> <span class="o">{</span>
  <span class="k">def</span> <span class="nf">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
    <span class="k">val</span> <span class="nv">logFile</span> <span class="k">=</span> <span class="s">"YOUR_SPARK_HOME/README.md"</span> <span class="c1">// Should be some file on your system
</span>    <span class="k">val</span> <span class="nv">spark</span> <span class="k">=</span> <span class="nv">SparkSession</span><span class="o">.</span><span class="py">builder</span><span class="o">.</span><span class="py">appName</span><span class="o">(</span><span class="s">"Simple Application"</span><span class="o">).</span><span class="py">getOrCreate</span><span class="o">()</span>
    <span class="k">val</span> <span class="nv">logData</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="n">logFile</span><span class="o">).</span><span class="py">cache</span><span class="o">()</span>
    <span class="k">val</span> <span class="nv">numAs</span> <span class="k">=</span> <span class="nv">logData</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">contains</span><span class="o">(</span><span class="s">"a"</span><span class="o">)).</span><span class="py">count</span><span class="o">()</span>
    <span class="k">val</span> <span class="nv">numBs</span> <span class="k">=</span> <span class="nv">logData</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="nv">line</span><span class="o">.</span><span class="py">contains</span><span class="o">(</span><span class="s">"b"</span><span class="o">)).</span><span class="py">count</span><span class="o">()</span>
    <span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="s">"Lines with a: $numAs, Lines with b: $numBs"</span><span class="o">)</span>
    <span class="nv">spark</span><span class="o">.</span><span class="py">stop</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

    <p>注意，这个应用程序我们应该定义一个 <code>main()</code> 方法而不是去继承scala.App<code>。使用 </code>scala.App` 的子类可能不会正常运行。</p>

    <p>该程序只计算Spark README文件中包含“ a”的行数和包含“ b”的行数。请注意，你需要将YOUR_SPARK_HOME替换为你自己电脑中Spark的安装路径。与前面带有Spark shell的示例（其初始化其自己的SparkSession）不同，我们将SparkSession初始化为程序的一部分。</p>

    <p>我们调用<code>SparkSession.builder</code>构造一个[[SparkSession]]，然后设置应用程序名称，最后调用<code>getOrCreate</code>以获得[[SparkSession]]实例。</p>

    <p>我们的应用依赖了 Spark API，所以我们将包含一个名为 <code>build.sbt</code> 的 sbt 配置文件，它描述了 Spark 的依赖。该文件也会添加一个 Spark 依赖的 repository:</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">name</span> <span class="o">:=</span> <span class="s">"Simple Project"</span>

<span class="n">version</span> <span class="o">:=</span> <span class="s">"1.0"</span>

<span class="n">scalaVersion</span> <span class="o">:=</span> <span class="s">"2.12.10"</span>

<span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">"org.apache.spark"</span> <span class="o">%%</span> <span class="s">"spark-sql"</span> <span class="o">%</span> <span class="s">"3.0.0-SNAPSHOT"</span></code></pre></figure>

    <p>为了让 sbt 正常运行，我们需要根据经典的目录结构来布局 <code>SimpleApp.scala</code> 和 <code>build.sbt</code> 文件。在成功后，我们可以创建一个包含应用程序代码的 JAR 包，然后使用 <code>spark-submit</code> 脚本来运行我们的程序。</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 你的目录结构应该是这样子的</span>
<span class="nv">$ </span>find <span class="nb">.</span>
<span class="nb">.</span>
./build.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

<span class="c"># 将你的程序打包</span>
<span class="nv">$ </span>sbt package
...
<span class="o">[</span>info] Packaging <span class="o">{</span>..<span class="o">}</span>/<span class="o">{</span>..<span class="o">}</span>/target/scala-2.12/simple-project_2.12-1.0.jar

<span class="c"># 使用spark-submit运行你的程序</span>
<span class="nv">$ </span>YOUR_SPARK_HOME/bin/spark-submit <span class="se">\</span>
  <span class="nt">--class</span> <span class="s2">"SimpleApp"</span> <span class="se">\</span>
  <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span>4] <span class="se">\</span>
  target/scala-2.12/simple-project_2.12-1.0.jar
...
Lines with a: 46, Lines with b: 23</code></pre></figure>

  </div>

<div data-lang="java">
    <p>本示例将使用Maven编译应用程序的JAR包，但是类似的构建工具都可以使用。</p>

    <p>我们将创建一个非常简单的Spark应用程序<code>SimpleApp.java</code>：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="cm">/* SimpleApp.java */</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">SimpleApp</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
    <span class="nc">String</span> <span class="n">logFile</span> <span class="o">=</span> <span class="s">"YOUR_SPARK_HOME/README.md"</span><span class="o">;</span> <span class="c1">// Should be some file on your system</span>
    <span class="nc">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="nc">SparkSession</span><span class="o">.</span><span class="na">builder</span><span class="o">().</span><span class="na">appName</span><span class="o">(</span><span class="s">"Simple Application"</span><span class="o">).</span><span class="na">getOrCreate</span><span class="o">();</span>
    <span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">logData</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">textFile</span><span class="o">(</span><span class="n">logFile</span><span class="o">).</span><span class="na">cache</span><span class="o">();</span>

    <span class="kt">long</span> <span class="n">numAs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">contains</span><span class="o">(</span><span class="s">"a"</span><span class="o">)).</span><span class="na">count</span><span class="o">();</span>
    <span class="kt">long</span> <span class="n">numBs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">contains</span><span class="o">(</span><span class="s">"b"</span><span class="o">)).</span><span class="na">count</span><span class="o">();</span>
    
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Lines with a: "</span> <span class="o">+</span> <span class="n">numAs</span> <span class="o">+</span> <span class="s">", lines with b: "</span> <span class="o">+</span> <span class="n">numBs</span><span class="o">);</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="na">stop</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

    <p>该程序只计算SparkREADME文件中包含“ a”的行数和包含“ b”的行数。请注意，你需要将YOUR_SPARK_HOME替换为自己电脑中Spark的安装路径。与前面带有Spark shell的示例（其初始化其自己的SparkSession）不同，我们将SparkSession初始化为程序的一部分。</p>

    <p>为了构建程序，我们还编写了一个Maven <code>pom.xml</code>文件，其中将Spark作为依赖项。请注意，Spark artifacts中带有Scala版本信息。</p>

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;project&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>edu.berkeley<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>simple-project<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;modelVersion&gt;</span>4.0.0<span class="nt">&lt;/modelVersion&gt;</span>
  <span class="nt">&lt;name&gt;</span>Simple Project<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;packaging&gt;</span>jar<span class="nt">&lt;/packaging&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;dependencies&gt;</span>
    <span class="nt">&lt;dependency&gt;</span> <span class="c">&lt;!-- Spark dependency --&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>spark-sql_2.12<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>3.0.0-SNAPSHOT<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
  <span class="nt">&lt;/dependencies&gt;</span>
<span class="nt">&lt;/project&gt;</span></code></pre></figure>

    <p>我们根据规范的Maven目录结构对这些文件进行布局：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>find <span class="nb">.</span>
./pom.xml
./src
./src/main
./src/main/java
./src/main/java/SimpleApp.java</code></pre></figure>

    <p>现在，我们可以使用Maven打包应用程序并使用<code>./bin/spark-submit</code>来执行它。</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 将你的应用程序打成jar包</span>
<span class="nv">$ </span>mvn package
...
<span class="o">[</span>INFO] Building jar: <span class="o">{</span>..<span class="o">}</span>/<span class="o">{</span>..<span class="o">}</span>/target/simple-project-1.0.jar

<span class="c"># 使用spark-submit运行你的程序</span>
<span class="nv">$ </span>YOUR_SPARK_HOME/bin/spark-submit <span class="se">\</span>
  <span class="nt">--class</span> <span class="s2">"SimpleApp"</span> <span class="se">\</span>
  <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span>4] <span class="se">\</span>
  target/simple-project-1.0.jar
...
Lines with a: 46, Lines with b: 23</code></pre></figure>

  </div>
<div data-lang="python">
    <p>现在，我们将展示如何使用Python API（PySpark）编写应用程序。</p>

    <p>如果要构建打包的PySpark应用程序或库，则可以按以下方式将其添加到setup.py文件中：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
        <span class="s">'pyspark=={site.SPARK_VERSION}'</span>
    <span class="p">]</span></code></pre></figure>

    <p>作为示例，我们将创建一个简单的Spark应用程序<code>SimpleApp.py</code>：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">"""SimpleApp.py"""</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">logFile</span> <span class="o">=</span> <span class="s">"YOUR_SPARK_HOME/README.md"</span>  <span class="c1"># Should be some file on your system
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"SimpleApp"</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">logData</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">logFile</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="n">numAs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">logData</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">'a'</span><span class="p">))</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">numBs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">logData</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">'b'</span><span class="p">))</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Lines with a: </span><span class="si">%</span><span class="s">i, lines with b: </span><span class="si">%</span><span class="s">i"</span> <span class="o">%</span> <span class="p">(</span><span class="n">numAs</span><span class="p">,</span> <span class="n">numBs</span><span class="p">))</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span></code></pre></figure>

    <p>该程序只计算文本文件中包含“ a”的行数和包含“ b”的行数。请注意，你需要将YOUR_SPARK_HOME替换为自己电脑Spark的安装路径。与Scala和Java示例一样，我们使用SparkSession创建DataSet。对于使用自定义类或第三方库的应用程序，我们还可以使用<code>spark-submit</code>将其打包到.zip文件中，同时使用参数<code>--py-files</code>指定代码。有关详细信息，请参见<code>spark-submit --help</code>。 <code>SimpleApp</code>非常简单，我们不需要指定任何代码依赖项。</p>

    <p>我们可以使用以下<code>bin/spark-submit</code>脚本运行此应用程序：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 使用spark-submit运行你的程序</span>
<span class="nv">$ </span>YOUR_SPARK_HOME/bin/spark-submit <span class="se">\</span>
  <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span>4] <span class="se">\</span>
  SimpleApp.py
...
Lines with a: 46, Lines with b: 23</code></pre></figure>

    <p>If you have PySpark pip installed into your environment (e.g., <code>pip install pyspark</code>), you can run your application with the regular Python interpreter or use the provided &#8216;spark-submit&#8217; as you prefer.</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 使用Python解释器运行你的程序</span>
<span class="nv">$ </span>python SimpleApp.py
...
Lines with a: 46, Lines with b: 23</code></pre></figure>

  </div>
</div>

<h1 id="下一步做什么">下一步做什么</h1>
<p>祝贺你运行了第一个Spark应用程序！</p>

<ul>
  <li>有关API的深入概述，请从<a href="rdd-programming-guide.html">RDD编程指南</a>和<a href="sql-programming-guide.html">SQL编程指南</a>开始，或参阅“编程指南”菜单以获取其他组件。</li>
  <li>要在集群上运行应用程序，请转至<a href="cluster-overview.html">部署概述</a>。</li>
  <li>最后，Spark在<code>examples</code>目录（<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples">Scala</a>， <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples">Java</a>， <a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python</a>， <a href="https://github.com/apache/spark/tree/master/examples/src/main/r">R</a>）中包含几个示例。你可以按下面的方式运行它们：</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 对于Scala和Java的示例，使用run-example：</span>
./bin/run-example SparkPi

<span class="c"># 对于Python的示例，直接使用 spark-submit：</span>
./bin/spark-submit examples/src/main/python/pi.py

<span class="c"># 对于R语言的示例，直接使用 spark-submit：</span>

./bin/spark-submit examples/src/main/r/dataframe.R</code></pre></figure>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
