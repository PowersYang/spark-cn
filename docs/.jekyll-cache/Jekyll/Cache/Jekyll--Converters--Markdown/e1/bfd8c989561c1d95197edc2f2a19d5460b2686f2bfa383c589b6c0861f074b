I"‘<p>Spark SQLæ˜¯ç”¨äºç»“æ„åŒ–æ•°æ®å¤„ç†çš„Sparkæ¨¡å—ã€‚ä¸åŸºç¡€çš„Spark RDD APIä¸åŒï¼ŒSpark SQLæä¾›çš„æ¥å£ä¸ºSparkæä¾›äº†æœ‰å…³æ•°æ®ç»“æ„å’Œæ‰§è¡Œè®¡ç®—çš„æ›´å¤šä¿¡æ¯ã€‚åœ¨å†…éƒ¨ï¼ŒSpark SQLä½¿ç”¨è¿™äº›é¢å¤–çš„ä¿¡æ¯æ¥æ‰§è¡Œé¢å¤–çš„ä¼˜åŒ–ã€‚ä¸Spark SQLäº¤äº’çš„æ–¹æ³•æœ‰å¤šç§ï¼ŒåŒ…æ‹¬SQLå’ŒDataset APIã€‚è®¡ç®—ç»“æœæ—¶ï¼Œå°†ä½¿ç”¨ç›¸åŒçš„æ‰§è¡Œå¼•æ“ï¼Œè€Œä¸è¦ç”¨æ¥è¡¨è¾¾è®¡ç®—çš„APIæˆ–è¯­è¨€æ— å…³ã€‚è¿™ç§ç»Ÿä¸€æ„å‘³ç€å¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°åœ¨ä¸åŒçš„APIä¹‹é—´æ¥å›åˆ‡æ¢ï¼Œä»è€Œæä¾›æœ€è‡ªç„¶çš„æ–¹å¼æ¥è¡¨è¾¾ç»™å®šçš„è½¬æ¢ã€‚</p>

<p>æ­¤é¡µé¢ä¸Šçš„æ‰€æœ‰ç¤ºä¾‹å‡ä½¿ç”¨Sparkå‘è¡Œç‰ˆä¸­åŒ…å«çš„ç¤ºä¾‹æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥åœ¨<code>spark-shell</code>ï¼Œ<code>pyspark shell</code>æˆ–<code>sparkR shell</code>ä¸­è¿è¡Œã€‚</p>

<h2 id="sql">SQL</h2>

<p>Spark SQLçš„ä¸€ç§ç”¨é€”æ˜¯æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚Spark SQLè¿˜å¯ä»¥ç”¨äºä»ç°æœ‰çš„Hiveä¸­è¯»å–æ•°æ®ã€‚æœ‰å…³å¦‚ä½•é…ç½®æ­¤åŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ<a href="http://spark-cn.cn/sql-data-sources-hive-tables.html">Hive Tables</a>éƒ¨åˆ†ã€‚å½“ä»å¦ä¸€ç§ç¼–ç¨‹è¯­è¨€ä¸­è¿è¡ŒSQLæ—¶ï¼Œç»“æœå°†ä½œä¸º<a href="http://spark-cn.cn/sql-programming-guide.html#datasets-and-dataframes">Dataset / DataFrame</a>è¿”å›ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨<a href="http://spark-cn.cn/sql-distributed-sql-engine.html#running-the-spark-sql-cli">å‘½ä»¤è¡Œ</a> æˆ–é€šè¿‡<a href="http://spark-cn.cn/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server">JDBC / ODBC</a>ä¸SQLæ¥å£è¿›è¡Œäº¤äº’ã€‚</p>

<h2 id="datasets-å’Œ-dataframe">Datasets å’Œ DataFrame</h2>

<p>Dataset æ˜¯æ•°æ®çš„åˆ†å¸ƒå¼é›†åˆã€‚Datasetæ˜¯Spark 1.6ä¸­æ·»åŠ çš„æ–°æ¥å£ï¼Œå®ƒå…·æœ‰RDDçš„ä¼˜ç‚¹ï¼ˆå¼ºç±»å‹è¾“å…¥ï¼Œä½¿ç”¨å¼ºå¤§çš„Lambdaå‡½æ•°çš„èƒ½åŠ›ï¼‰å’ŒSpark SQLçš„ä¼˜åŒ–æ‰§è¡Œå¼•æ“çš„ä¼˜ç‚¹ã€‚Datasetå¯ä»¥è¢«ä»JVMå¯¹è±¡ä¸­<a href="http://spark-cn.cn/sql-getting-started.html#creating-datasets">æ„é€ </a>ï¼Œç„¶åä½¿ç”¨å‡½æ•°è½¬æ¢ï¼ˆ<code>map</code>ï¼Œ<code>flatMap</code>ï¼Œ<code>filter</code>ç­‰ç­‰ï¼‰ã€‚Dataset APIåœ¨<a href="http://spark-cn.cn/api/scala/index.html#org.apache.spark.sql.Dataset">Scala</a>å’Œ <a href="http://spark-cn.cn/api/java/index.html?org/apache/spark/sql/Dataset.html">Javaä¸­</a>å¯ç”¨ã€‚Pythonä¸æ”¯æŒDataset APIã€‚ä½†æ˜¯ç”±äºPythonçš„åŠ¨æ€ç‰¹æ€§ï¼ŒDataset APIçš„è®¸å¤šä¼˜ç‚¹å·²ç»å¯ç”¨ï¼ˆå³ï¼Œæ‚¨å¯ä»¥è‡ªç„¶åœ°é€šè¿‡åç§°è®¿é—®è¡Œå­—æ®µ <code>row.columnName</code>ï¼‰ã€‚Rçš„æƒ…å†µç±»ä¼¼ã€‚</p>

<p>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually
equivalent to a table in a relational database or a data frame in R/Python, but with richer
optimizations under the hood. DataFrames can be constructed from a wide array of <a href="sql-data-sources.html">sources</a> such
as: structured data files, tables in Hive, external databases, or existing RDDs.
The DataFrame API is available in Scala,
Java, <a href="api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a>, and <a href="api/R/index.html">R</a>.
In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s.
In <a href="api/scala/index.html#org.apache.spark.sql.Dataset">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>.
While, in <a href="api/java/index.html?org/apache/spark/sql/Dataset.html">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p>

<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p>
:ET