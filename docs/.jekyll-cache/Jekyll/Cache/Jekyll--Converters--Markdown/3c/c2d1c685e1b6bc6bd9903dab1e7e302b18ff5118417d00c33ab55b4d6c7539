I"7<ul id="markdown-toc">
  <li><a href="#upgrading-from-sparkr-24-to-30" id="markdown-toc-upgrading-from-sparkr-24-to-30">Upgrading from SparkR 2.4 to 3.0</a></li>
  <li><a href="#upgrading-from-sparkr-23-to-24" id="markdown-toc-upgrading-from-sparkr-23-to-24">Upgrading from SparkR 2.3 to 2.4</a></li>
  <li><a href="#upgrading-from-sparkr-23-to-231-and-above" id="markdown-toc-upgrading-from-sparkr-23-to-231-and-above">Upgrading from SparkR 2.3 to 2.3.1 and above</a></li>
  <li><a href="#upgrading-from-sparkr-22-to-23" id="markdown-toc-upgrading-from-sparkr-22-to-23">Upgrading from SparkR 2.2 to 2.3</a></li>
  <li><a href="#upgrading-from-sparkr-21-to-22" id="markdown-toc-upgrading-from-sparkr-21-to-22">Upgrading from SparkR 2.1 to 2.2</a></li>
  <li><a href="#upgrading-from-sparkr-20-to-31" id="markdown-toc-upgrading-from-sparkr-20-to-31">Upgrading from SparkR 2.0 to 3.1</a></li>
  <li><a href="#upgrading-from-sparkr-16-to-20" id="markdown-toc-upgrading-from-sparkr-16-to-20">Upgrading from SparkR 1.6 to 2.0</a></li>
  <li><a href="#upgrading-from-sparkr-15-to-16" id="markdown-toc-upgrading-from-sparkr-15-to-16">Upgrading from SparkR 1.5 to 1.6</a></li>
</ul>

<p>Note that this migration guide describes the items specific to SparkR.
Many items of SQL migration can be applied when migrating SparkR to higher versions.
Please refer <a href="sql-migration-guide.html">Migration Guide: SQL, Datasets and DataFrame</a>.</p>

<h2 id="upgrading-from-sparkr-24-to-30">Upgrading from SparkR 2.4 to 3.0</h2>

<ul>
  <li>The deprecated methods <code>sparkR.init</code>, <code>sparkRSQL.init</code>, <code>sparkRHive.init</code> have been removed. Use <code>sparkR.session</code> instead.</li>
  <li>The deprecated methods <code>parquetFile</code>, <code>saveAsParquetFile</code>, <code>jsonFile</code>, <code>registerTempTable</code>, <code>createExternalTable</code>, and <code>dropTempTable</code> have been removed. Use <code>read.parquet</code>, <code>write.parquet</code>, <code>read.json</code>, <code>createOrReplaceTempView</code>, <code>createTable</code>, <code>dropTempView</code>, <code>union</code> instead.</li>
</ul>

<h2 id="upgrading-from-sparkr-23-to-24">Upgrading from SparkR 2.3 to 2.4</h2>

<ul>
  <li>Previously, we don&#8217;t check the validity of the size of the last layer in <code>spark.mlp</code>. For example, if the training data only has two labels, a <code>layers</code> param like <code>c(1, 3)</code> doesn&#8217;t cause an error previously, now it does.</li>
</ul>

<h2 id="upgrading-from-sparkr-23-to-231-and-above">Upgrading from SparkR 2.3 to 2.3.1 and above</h2>

<ul>
  <li>In SparkR 2.3.0 and earlier, the <code>start</code> parameter of <code>substr</code> method was wrongly subtracted by one and considered as 0-based. This can lead to inconsistent substring results and also does not match with the behaviour with <code>substr</code> in R. In version 2.3.1 and later, it has been fixed so the <code>start</code> parameter of <code>substr</code> method is now 1-based. As an example, <code>substr(lit('abcdef'), 2, 4))</code> would result to <code>abc</code> in SparkR 2.3.0, and the result would be <code>bcd</code> in SparkR 2.3.1.</li>
</ul>

<h2 id="upgrading-from-sparkr-22-to-23">Upgrading from SparkR 2.2 to 2.3</h2>

<ul>
  <li>The <code>stringsAsFactors</code> parameter was previously ignored with <code>collect</code>, for example, in <code>collect(createDataFrame(iris), stringsAsFactors = TRUE))</code>. It has been corrected.</li>
  <li>For <code>summary</code>, option for statistics to compute has been added. Its output is changed from that from <code>describe</code>.</li>
  <li>A warning can be raised if versions of SparkR package and the Spark JVM do not match.</li>
</ul>

<h2 id="upgrading-from-sparkr-21-to-22">Upgrading from SparkR 2.1 to 2.2</h2>

<ul>
  <li>A <code>numPartitions</code> parameter has been added to <code>createDataFrame</code> and <code>as.DataFrame</code>. When splitting the data, the partition position calculation has been made to match the one in Scala.</li>
  <li>The method <code>createExternalTable</code> has been deprecated to be replaced by <code>createTable</code>. Either methods can be called to create external or managed table. Additional catalog methods have also been added.</li>
  <li>By default, derby.log is now saved to <code>tempdir()</code>. This will be created when instantiating the SparkSession with <code>enableHiveSupport</code> set to <code>TRUE</code>.</li>
  <li><code>spark.lda</code> was not setting the optimizer correctly. It has been corrected.</li>
  <li>Several model summary outputs are updated to have <code>coefficients</code> as <code>matrix</code>. This includes <code>spark.logit</code>, <code>spark.kmeans</code>, <code>spark.glm</code>. Model summary outputs for <code>spark.gaussianMixture</code> have added log-likelihood as <code>loglik</code>.</li>
</ul>

<h2 id="upgrading-from-sparkr-20-to-31">Upgrading from SparkR 2.0 to 3.1</h2>

<ul>
  <li><code>join</code> no longer performs Cartesian Product by default, use <code>crossJoin</code> instead.</li>
</ul>

<h2 id="upgrading-from-sparkr-16-to-20">Upgrading from SparkR 1.6 to 2.0</h2>

<ul>
  <li>The method <code>table</code> has been removed and replaced by <code>tableToDF</code>.</li>
  <li>The class <code>DataFrame</code> has been renamed to <code>SparkDataFrame</code> to avoid name conflicts.</li>
  <li>Spark&#8217;s <code>SQLContext</code> and <code>HiveContext</code> have been deprecated to be replaced by <code>SparkSession</code>. Instead of <code>sparkR.init()</code>, call <code>sparkR.session()</code> in its place to instantiate the SparkSession. Once that is done, that currently active SparkSession will be used for SparkDataFrame operations.</li>
  <li>The parameter <code>sparkExecutorEnv</code> is not supported by <code>sparkR.session</code>. To set environment for the executors, set Spark config properties with the prefix &#8220;spark.executorEnv.VAR_NAME&#8221;, for example, &#8220;spark.executorEnv.PATH&#8221;</li>
  <li>The <code>sqlContext</code> parameter is no longer required for these functions: <code>createDataFrame</code>, <code>as.DataFrame</code>, <code>read.json</code>, <code>jsonFile</code>, <code>read.parquet</code>, <code>parquetFile</code>, <code>read.text</code>, <code>sql</code>, <code>tables</code>, <code>tableNames</code>, <code>cacheTable</code>, <code>uncacheTable</code>, <code>clearCache</code>, <code>dropTempTable</code>, <code>read.df</code>, <code>loadDF</code>, <code>createExternalTable</code>.</li>
  <li>The method <code>registerTempTable</code> has been deprecated to be replaced by <code>createOrReplaceTempView</code>.</li>
  <li>The method <code>dropTempTable</code> has been deprecated to be replaced by <code>dropTempView</code>.</li>
  <li>The <code>sc</code> SparkContext parameter is no longer required for these functions: <code>setJobGroup</code>, <code>clearJobGroup</code>, <code>cancelJobGroup</code></li>
</ul>

<h2 id="upgrading-from-sparkr-15-to-16">Upgrading from SparkR 1.5 to 1.6</h2>

<ul>
  <li>Before Spark 1.6.0, the default mode for writes was <code>append</code>. It was changed in Spark 1.6.0 to <code>error</code> to match the Scala API.</li>
  <li>SparkSQL converts <code>NA</code> in R to <code>null</code> and vice-versa.</li>
  <li>Since 1.6.1, withColumn method in SparkR supports adding a new column to or replacing existing columns
of the same name of a DataFrame.</li>
</ul>
:ET