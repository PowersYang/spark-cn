(window.webpackJsonp=window.webpackJsonp||[]).push([[217],{417:function(e,t,a){"use strict";a.r(t);var s=a(0),o=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"c-api-libhdfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#c-api-libhdfs"}},[e._v("#")]),e._v(" C API libhdfs")]),e._v(" "),a("h2",{attrs:{id:"overview"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),a("p",[e._v("libhdfs is a JNI based C API for Hadoop’s Distributed File System (HDFS). It provides C APIs to a subset of the HDFS APIs to manipulate HDFS files and the filesystem. libhdfs is part of the Hadoop distribution and comes pre-compiled in $HADOOP_HDFS_HOME/lib/native/libhdfs.so . libhdfs is compatible with Windows and can be built on Windows by running mvn compile within the hadoop-hdfs-project/hadoop-hdfs directory of the source tree.")]),e._v(" "),a("h2",{attrs:{id:"the-apis"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-apis"}},[e._v("#")]),e._v(" The APIs")]),e._v(" "),a("p",[e._v("The libhdfs APIs are a subset of the "),a("router-link",{attrs:{to:"/en/docs/api/org/apache/hadoop/fs/FileSystem.html"}},[e._v("Hadoop FileSystem APIs")]),e._v(".")],1),e._v(" "),a("p",[e._v("The header file for libhdfs describes each API in detail and is available in $HADOOP_HDFS_HOME/include/hdfs.h.")]),e._v(" "),a("h2",{attrs:{id:"a-sample-program"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#a-sample-program"}},[e._v("#")]),e._v(" A Sample Program")]),e._v(" "),a("pre",[a("code",[e._v('#include "hdfs.h"\n\nint main(int argc, char **argv) {\n\n    hdfsFS fs = hdfsConnect("default", 0);\n    const char* writePath = "/tmp/testfile.txt";\n    hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY |O_CREAT, 0, 0, 0);\n    if(!writeFile) {\n          fprintf(stderr, "Failed to open %s for writing!\\n", writePath);\n          exit(-1);\n    }\n    char* buffer = "Hello, World!";\n    tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer)+1);\n    if (hdfsFlush(fs, writeFile)) {\n           fprintf(stderr, "Failed to \'flush\' %s\\n", writePath);\n          exit(-1);\n    }\n    hdfsCloseFile(fs, writeFile);\n}\n')])]),e._v(" "),a("h2",{attrs:{id:"how-to-link-with-the-library"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#how-to-link-with-the-library"}},[e._v("#")]),e._v(" How To Link With The Library")]),e._v(" "),a("p",[e._v("See the CMake file for test_libhdfs_ops.c in the libhdfs source directory (hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt) or something like: gcc above_sample.c -I$HADOOP_HDFS_HOME/include -L$HADOOP_HDFS_HOME/lib/native -lhdfs -o above_sample")]),e._v(" "),a("h2",{attrs:{id:"common-problems"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#common-problems"}},[e._v("#")]),e._v(" Common Problems")]),e._v(" "),a("p",[e._v("The most common problem is the CLASSPATH is not set properly when calling a program that uses libhdfs. Make sure you set it to all the Hadoop jars needed to run Hadoop itself as well as the right configuration directory containing hdfs-site.xml. It is not valid to use wildcard syntax for specifying multiple jars. It may be useful to run hadoop classpath --glob or hadoop classpath --jar "),a("path",[e._v(" to generate the correct classpath for your deployment. See "),a("router-link",{attrs:{to:"/en/docs/hadoop-project-dist/hadoop-common/CommandsManual.html#classpath"}},[e._v("Hadoop Commands Reference")]),e._v(" for more information on this command.")],1)]),e._v(" "),a("h2",{attrs:{id:"thread-safe"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#thread-safe"}},[e._v("#")]),e._v(" Thread Safe")]),e._v(" "),a("p",[e._v("libdhfs is thread safe.")]),e._v(" "),a("ul",[a("li",[e._v("Concurrency and Hadoop FS “handles”")])]),e._v(" "),a("p",[e._v("The Hadoop FS implementation includes an FS handle cache which caches based on the URI of the namenode along with the user connecting. So, all calls to hdfsConnect will return the same handle but calls to hdfsConnectAsUser with different users will return different handles. But, since HDFS client handles are completely thread safe, this has no bearing on concurrency.")]),e._v(" "),a("ul",[a("li",[e._v("Concurrency and libhdfs/JNI")])]),e._v(" "),a("p",[e._v("The libhdfs calls to JNI should always be creating thread local storage, so (in theory), libhdfs should be as thread safe as the underlying calls to the Hadoop FS.")])])}),[],!1,null,null,null);t.default=o.exports}}]);