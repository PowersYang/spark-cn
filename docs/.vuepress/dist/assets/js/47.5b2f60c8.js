(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{247:function(e,a,t){"use strict";t.r(a);var i=t(0),o=Object(i.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"native-libraries-guide"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#native-libraries-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Native Libraries Guide")]),e._v(" "),t("h2",{attrs:{id:"overview"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#overview","aria-hidden":"true"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),t("p",[e._v("This guide describes the native hadoop library and includes a small discussion about native shared libraries.")]),e._v(" "),t("p",[e._v("Note: Depending on your environment, the term “native libraries” could refer to all *.so’s you need to compile; and, the term “native compression” could refer to all *.so’s you need to compile that are specifically related to compression. Currently, however, this document only addresses the native hadoop library (libhadoop.so). The document for libhdfs library (libhdfs.so) is "),t("router-link",{attrs:{to:"/docs/hadoop-project-dist/hadoop-hdfs/LibHdfs.html"}},[e._v("here")]),e._v(".")],1),e._v(" "),t("h2",{attrs:{id:"native-hadoop-library"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#native-hadoop-library","aria-hidden":"true"}},[e._v("#")]),e._v(" Native Hadoop Library")]),e._v(" "),t("p",[e._v("Hadoop has native implementations of certain components for performance reasons and for non-availability of Java implementations. These components are available in a single, dynamically-linked native library called the native hadoop library. On the *nix platforms the library is named libhadoop.so.")]),e._v(" "),t("h2",{attrs:{id:"usage"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#usage","aria-hidden":"true"}},[e._v("#")]),e._v(" Usage")]),e._v(" "),t("p",[e._v("It is fairly easy to use the native hadoop library:")]),e._v(" "),t("ol",[t("li",[e._v("Review the components.")]),e._v(" "),t("li",[e._v("Review the supported platforms.")]),e._v(" "),t("li",[e._v("Either download a hadoop release, which will include a pre-built version of the native hadoop library, or build your own version of the native hadoop library. Whether you download or build, the name for the library is the same: libhadoop.so")]),e._v(" "),t("li",[e._v("Install the compression codec development packages (>zlib-1.2, >gzip-1.2):")])]),e._v(" "),t("pre",[t("code",[e._v("* If you download the library, install one or more development packages - whichever compression codecs you want to use with your deployment.\n* If you build the library, it is mandatory to install both development packages.\n")])]),e._v(" "),t("ol",{attrs:{start:"5"}},[t("li",[e._v("Check the runtime log files.")])]),e._v(" "),t("h2",{attrs:{id:"components"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#components","aria-hidden":"true"}},[e._v("#")]),e._v(" Components")]),e._v(" "),t("p",[e._v("The native hadoop library includes various components:")]),e._v(" "),t("ul",[t("li",[e._v("Compression Codecs (bzip2, lz4, snappy, zlib)")]),e._v(" "),t("li",[e._v("Native IO utilities for "),t("router-link",{attrs:{to:"/docs/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html"}},[e._v("HDFS Short-Circuit Local Reads")]),e._v(" and "),t("router-link",{attrs:{to:"/docs/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html"}},[e._v("Centralized Cache Management in HDFS")])],1),e._v(" "),t("li",[e._v("CRC32 checksum implementation")])]),e._v(" "),t("h2",{attrs:{id:"supported-platforms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#supported-platforms","aria-hidden":"true"}},[e._v("#")]),e._v(" Supported Platforms")]),e._v(" "),t("p",[e._v("The native hadoop library is supported on *nix platforms only. The library does not to work with Cygwin or the Mac OS X platform.")]),e._v(" "),t("p",[e._v("The native hadoop library is mainly used on the GNU/Linus platform and has been tested on these distributions:")]),e._v(" "),t("ul",[t("li",[e._v("RHEL4/Fedora")]),e._v(" "),t("li",[e._v("Ubuntu")]),e._v(" "),t("li",[e._v("Gentoo")])]),e._v(" "),t("p",[e._v("On all the above distributions a 32/64 bit native hadoop library will work with a respective 32/64 bit jvm.")]),e._v(" "),t("h2",{attrs:{id:"download"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#download","aria-hidden":"true"}},[e._v("#")]),e._v(" Download")]),e._v(" "),t("p",[e._v("The pre-built 32-bit i386-Linux native hadoop library is available as part of the hadoop distribution and is located in the lib/native directory. You can download the hadoop distribution from Hadoop Common Releases.")]),e._v(" "),t("p",[e._v("Be sure to install the zlib and/or gzip development packages - whichever compression codecs you want to use with your deployment.")]),e._v(" "),t("h2",{attrs:{id:"build"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#build","aria-hidden":"true"}},[e._v("#")]),e._v(" Build")]),e._v(" "),t("p",[e._v("The native hadoop library is written in ANSI C and is built using the GNU autotools-chain (autoconf, autoheader, automake, autoscan, libtool). This means it should be straight-forward to build the library on any platform with a standards-compliant C compiler and the GNU autotools-chain (see the supported platforms).")]),e._v(" "),t("p",[e._v("The packages you need to install on the target platform are:")]),e._v(" "),t("ul",[t("li",[e._v("C compiler (e.g. GNU C Compiler)")]),e._v(" "),t("li",[e._v("GNU Autools Chain: autoconf, automake, libtool")]),e._v(" "),t("li",[e._v("zlib-development package (stable version >= 1.2.0)")]),e._v(" "),t("li",[e._v("openssl-development package(e.g. libssl-dev)")])]),e._v(" "),t("p",[e._v("Once you installed the prerequisite packages use the standard hadoop pom.xml file and pass along the native flag to build the native hadoop library:")]),e._v(" "),t("pre",[t("code",[e._v("   $ mvn package -Pdist,native -DskipTests -Dtar\n")])]),e._v(" "),t("p",[e._v("You should see the newly-built library in:")]),e._v(" "),t("pre",[t("code",[e._v("   $ hadoop-dist/target/hadoop-3.2.1/lib/native\n")])]),e._v(" "),t("p",[e._v("Please note the following:")]),e._v(" "),t("ul",[t("li",[e._v("It is mandatory to install both the zlib and gzip development packages on the target platform in order to build the native hadoop library; however, for deployment it is sufficient to install just one package if you wish to use only one codec.")]),e._v(" "),t("li",[e._v("It is necessary to have the correct 32/64 libraries for zlib, depending on the 32/64 bit jvm for the target platform, in order to build and deploy the native hadoop library.")])]),e._v(" "),t("h2",{attrs:{id:"runtime"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#runtime","aria-hidden":"true"}},[e._v("#")]),e._v(" Runtime")]),e._v(" "),t("p",[e._v("The bin/hadoop script ensures that the native hadoop library is on the library path via the system property: -Djava.library.path="),t("path")]),e._v(" "),t("p",[e._v("During runtime, check the hadoop log files for your MapReduce tasks.")]),e._v(" "),t("ul",[t("li",[e._v("If everything is all right, then: DEBUG util.NativeCodeLoader - Trying to load the custom-built native-hadoop library... INFO util.NativeCodeLoader - Loaded the native-hadoop library")]),e._v(" "),t("li",[e._v("If something goes wrong, then: INFO util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable")])]),e._v(" "),t("h2",{attrs:{id:"check"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#check","aria-hidden":"true"}},[e._v("#")]),e._v(" Check")]),e._v(" "),t("p",[e._v("NativeLibraryChecker is a tool to check whether native libraries are loaded correctly. You can launch NativeLibraryChecker as follows:")]),e._v(" "),t("pre",[t("code",[e._v("   $ hadoop checknative -a\n   14/12/06 01:30:45 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version\n   14/12/06 01:30:45 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\n   Native library checking:\n   hadoop: true /home/ozawa/hadoop/lib/native/libhadoop.so.1.0.0\n   zlib:   true /lib/x86_64-linux-gnu/libz.so.1\n   snappy: true /usr/lib/libsnappy.so.1\n   zstd: true /usr/lib/libzstd.so.1\n   lz4:    true revision:99\n   bzip2:  false\n")])]),e._v(" "),t("h2",{attrs:{id:"native-shared-libraries"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#native-shared-libraries","aria-hidden":"true"}},[e._v("#")]),e._v(" Native Shared Libraries")]),e._v(" "),t("p",[e._v("You can load any native shared library using DistributedCache for distributing and symlinking the library files.")]),e._v(" "),t("p",[e._v("This example shows you how to distribute a shared library, mylib.so, and load it from a MapReduce task.")]),e._v(" "),t("ol",[t("li",[e._v("First copy the library to the HDFS: bin/hadoop fs -copyFromLocal mylib.so.1 /libraries/mylib.so.1")]),e._v(" "),t("li",[e._v('The job launching program should contain the following: DistributedCache.createSymlink(conf); DistributedCache.addCacheFile("hdfs://host:port/libraries/mylib.so. 1#mylib.so", conf);')]),e._v(" "),t("li",[e._v('The MapReduce task can contain: System.loadLibrary("mylib.so");')])]),e._v(" "),t("p",[e._v("Note: If you downloaded or built the native hadoop library, you don’t need to use DistibutedCache to make the library available to your MapReduce tasks.")])])}),[],!1,null,null,null);a.default=o.exports}}]);