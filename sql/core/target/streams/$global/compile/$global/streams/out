[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/WriteToMicroBatchDataSource.scala:36: class WriteToDataSourceV2 in package v2 is deprecated (since 2.4.0): Use specific logical plans like AppendData instead[0m
[0m[[33mwarn[0m] [0m  def createPlan(batchId: Long): WriteToDataSourceV2 = {[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/WriteToMicroBatchDataSource.scala:37: class WriteToDataSourceV2 in package v2 is deprecated (since 2.4.0): Use specific logical plans like AppendData instead[0m
[0m[[33mwarn[0m] [0m    WriteToDataSourceV2(new MicroBatchWrite(batchId, write), query)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:128: value ENABLE_JOB_SUMMARY in class ParquetOutputFormat is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      && conf.get(ParquetOutputFormat.ENABLE_JOB_SUMMARY) == null) {[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:262: class ParquetInputSplit in package hadoop is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m        new org.apache.parquet.hadoop.ParquetInputSplit([0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:273: method readFooter in class ParquetFileReader is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m        ParquetFileReader.readFooter(sharedConf, filePath, SKIP_ROW_GROUPS).getFileMetaData[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:443: method readFooter in class ParquetFileReader is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m          ParquetFileReader.readFooter([0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetWriteBuilder.scala:91: value ENABLE_JOB_SUMMARY in class ParquetOutputFormat is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      && conf.get(ParquetOutputFormat.ENABLE_JOB_SUMMARY) == null) {[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala:120: class ParquetInputSplit in package hadoop is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m          Option[TimeZone]) => RecordReader[Void, T]): RecordReader[Void, T] = {[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala:125: class ParquetInputSplit in package hadoop is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      new org.apache.parquet.hadoop.ParquetInputSplit([0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala:134: method readFooter in class ParquetFileReader is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      ParquetFileReader.readFooter(conf, filePath, SKIP_ROW_GROUPS).getFileMetaData[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala:183: class ParquetInputSplit in package hadoop is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      split: ParquetInputSplit,[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/PowerYang/Documents/GitHub/spark-cn/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala:212: class ParquetInputSplit in package hadoop is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      split: ParquetInputSplit,[0m
[0m[[33mwarn[0m] [0m[0m
